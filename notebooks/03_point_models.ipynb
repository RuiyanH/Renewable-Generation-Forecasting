{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03 â€“ Point models\n",
        "\n",
        "Train baselines and a tree model; compare MAE/RMSE on a holdout slice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure project root is importable dynamically\n",
        "import sys, os\n",
        "from pathlib import Path\n",
        "\n",
        "def find_project_root(start: Path | None = None) -> Path:\n",
        "    if start is None:\n",
        "        start = Path.cwd()\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / \"src\").is_dir() and (p / \"requirements.txt\").exists():\n",
        "            return p\n",
        "        if (p / \".git\").exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "PROJECT_ROOT = find_project_root()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Could not infer datetime column name in CSV.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m     eng = pd.read_parquet(FEAT_PATH)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     df = \u001b[43mload_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAW_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     eng = add_calendar_features(df)\n\u001b[32m     26\u001b[39m     eng = add_lagged_load_features(eng, target_col=\u001b[33m\"\u001b[39m\u001b[33mload_mw\u001b[39m\u001b[33m\"\u001b[39m, lag_hours=(\u001b[32m1\u001b[39m, \u001b[32m24\u001b[39m, \u001b[32m168\u001b[39m), rolling_windows=(\u001b[32m24\u001b[39m, \u001b[32m168\u001b[39m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/forecast/src/data.py:51\u001b[39m, in \u001b[36mload_time_series\u001b[39m\u001b[34m(path, table)\u001b[39m\n\u001b[32m     49\u001b[39m \tdt_col = _infer_datetime_column_name(df.columns)\n\u001b[32m     50\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m dt_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \t\t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCould not infer datetime column name in CSV.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m _coerce_to_hourly_index(df, dt_col)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lower_path.endswith(\u001b[33m\"\u001b[39m\u001b[33m.sqlite\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m lower_path.endswith(\u001b[33m\"\u001b[39m\u001b[33m.db\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[31mValueError\u001b[39m: Could not infer datetime column name in CSV."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "from pathlib import Path\n",
        "from src.data import load_time_series, rename_region_columns_to_standard\n",
        "from src.features import add_calendar_features, add_lagged_load_features, select_feature_columns\n",
        "from src.models import (\n",
        "    baseline_persistence,\n",
        "    baseline_same_hour_last_week,\n",
        "    train_point_models,\n",
        "    evaluate_point_models,\n",
        ")\n",
        "from src.evaluation import summarize_metrics\n",
        "\n",
        "RAW_PATH = (PROJECT_ROOT / \"time_series_60min_singleindex.csv\")\n",
        "FEAT_PATH = (PROJECT_ROOT / \"data\" / \"processed_features.parquet\")\n",
        "DATETIME_COL = \"utc_timestamp\"\n",
        "\n",
        "# Load engineered features if available, else build quickly here\n",
        "if FEAT_PATH.exists():\n",
        "    eng = pd.read_parquet(FEAT_PATH)\n",
        "else:\n",
        "    df = load_time_series(str(RAW_PATH), datetime_col=DATETIME_COL)\n",
        "    df = rename_region_columns_to_standard(df, region=\"DE\")\n",
        "    if \"load_mw\" not in df.columns:\n",
        "        raise ValueError(\"Could not find Germany load column; verify source columns.\")\n",
        "    eng = add_calendar_features(df)\n",
        "    eng = add_lagged_load_features(eng, target_col=\"load_mw\", lag_hours=(1, 24, 168), rolling_windows=(24, 168))\n",
        "    eng = eng.dropna()\n",
        "\n",
        "X, y, feature_names = select_feature_columns(eng, target_col=\"load_mw\")\n",
        "print(\"Data ready:\", X.shape, \"features=\", len(feature_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-based train/validation split (last 20% as validation)\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_valid = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "y_train, y_valid = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "\n",
        "models = train_point_models(X_train, y_train)\n",
        "\n",
        "# Baselines using only the target history (applied on validation index)\n",
        "persist_pred = baseline_persistence(y, horizon_hours=1).iloc[split_idx:]\n",
        "weekly_pred = baseline_same_hour_last_week(y).iloc[split_idx:]\n",
        "\n",
        "# Drop any NA alignment\n",
        "mask = (~persist_pred.isna()) & (~weekly_pred.isna())\n",
        "persist_pred = persist_pred[mask]\n",
        "weekly_pred = weekly_pred[mask]\n",
        "y_valid_baselines = y_valid.loc[mask.index][mask]\n",
        "\n",
        "# Evaluate ML models (already aligned)\n",
        "results = evaluate_point_models(models, X_valid, y_valid)\n",
        "\n",
        "# Evaluate baselines\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "results[\"baseline_persistence\"] = {\n",
        "    \"MAE\": float(mean_absolute_error(y_valid_baselines, persist_pred)),\n",
        "    \"RMSE\": float(np.sqrt(mean_squared_error(y_valid_baselines, persist_pred)))\n",
        "}\n",
        "results[\"baseline_weekly\"] = {\n",
        "    \"MAE\": float(mean_absolute_error(y_valid_baselines, weekly_pred)),\n",
        "    \"RMSE\": float(np.sqrt(mean_squared_error(y_valid_baselines, weekly_pred)))\n",
        "}\n",
        "\n",
        "summ = summarize_metrics(results)\n",
        "summ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize a recent window of predictions vs truth\n",
        "plot_slice = y_valid.index[-7*24:]  # last week of validation\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 5))\n",
        "ax.plot(y_valid.loc[plot_slice].index, y_valid.loc[plot_slice].values, label=\"truth\", lw=1.5)\n",
        "ax.plot(y_valid.loc[plot_slice].index, models.gradient_boosting.predict(X_valid.loc[plot_slice]), label=\"GBR\", lw=1.2)\n",
        "ax.plot(y_valid.loc[plot_slice].index, models.random_forest.predict(X_valid.loc[plot_slice]), label=\"RF\", lw=1.0)\n",
        "ax.legend()\n",
        "ax.set_title(\"Validation window: predictions vs truth\")\n",
        "ax.set_xlabel(\"\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(str(PROJECT_ROOT / \"figures\" / \"point_models_validation_window.png\"), dpi=150)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (forecast)",
      "language": "python",
      "name": "forecast-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
